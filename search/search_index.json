{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"DepthFlow Images to \u2192 3D Parallax effect video \u2022 A free and open source ImmersityAI alternative         Links \u2022     Installation \u2022     Documentation \u2022     ComfyUI \u2022     Issues \u2022     Funding <sub> YouTube \u2022     GitHub \u2022     Contact \u2022     Changelog \u2022     License </sub>"},{"location":"#description","title":"\ud83d\udd25 Description","text":"<p>DepthFlow is an advanced image-to-video converter that transforms static pictures into awesome 3D parallax animations. Bring photos to life with motion, featuring high quality and custom presets, perfect for digital art, social media, stock footage, fillers and more.</p> <ul> <li> High quality results with seamless loops and artifact-free edges, ensuring a polished and professional look for your animations. Enhance your creations with upscalers and add a touch of magic with lens distortion, depth of field, vignette post effects!</li> <li> Fast processing with an heavily optimized GLSL Shader running on the GPU. Render up to 8k50fps with an RTX 3060, export videos with any resolution, codec, supersampling.</li> <li> Commercial use is encouraged \u2022 Kindly retribute back if you got value from it \u2764\ufe0f</li> <li> Powerful WebUI built with Gradio, for an user-friendly experience:</li> </ul> <p></p> <ul> <li> Use your own depthmaps, or let them be estimated with the latest AI models available!</li> <li> Customizable with a wide range of projection parameters, allowing you to precisely tweak the effect to your liking. Automate it with Python scripts for mass production!</li> <li> Self hosted with no watermarks, unlimited usage, portable ready-to-run executables. It's free and open source, no strings attached, as opposed to competitors \ud83d\ude04</li> </ul> <p>\u2764\ufe0f Loving it? Your support is essential!</p> <p></p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Head out to the Official Website for the latest installation instructions and more!</p> <p> </p>"},{"location":"#usage","title":"\u2b50\ufe0f Usage","text":"<p>See all Quick Start options in the website as well!</p> <p> </p>"},{"location":"#community","title":"\u267b\ufe0f Community","text":"<p>\u2705 Be featured here if you're using DepthFlow in your projects or art, get in touch!</p> <p>Check out amazing community work built on top of DepthFlow:</p> <ul> <li>\u2b50\ufe0f ComfyUI Node Pack by @akatz-ai, also in CivitAI</li> </ul>"},{"location":"about/changelog/","title":"Changelog","text":""},{"location":"about/changelog/#0.9.2","title":"\u270f\ufe0f v0.9.2 August ??, 2025","text":"<ul> <li>Fixed a internal resolution doubling bug before the final resize</li> <li>Recalled all executable releases, enough users didn't see warnings</li> <li>Fixed FFmpeg command line interface options missing</li> <li>Fix <code>turbopipe.sync</code> shouldn't be called when disabled</li> <li>Make the inpainting mask color white</li> </ul>"},{"location":"about/changelog/#0.9.0","title":"\ud83d\udce6 v0.9.0 June 2, 2025","text":"<ul> <li>Convert the project into snake case, still have my differences</li> <li>Overhauled the Readme and the WebUI layout and content</li> <li>Improvements to perceptual quality of the animation presets</li> <li>Add Upscayl as an upscaler option</li> <li>Add a command line interface for all upscalers and depth estimators</li> <li>Fixed drag and drop of files due new lazy loading logic</li> <li>Add stretch detection math on the shader for potential fill-in with gen ai</li> <li>Add colors filters (sepia, grayscale, saturation, contrast, brightness)</li> <li>Add transverse lens distortion filter (intensity, decay options)</li> <li>Overhaul animation system to be more flexible and reliable</li> <li>Reorganize website examples section into their own pages</li> <li>Cached depthmaps are now handled by <code>diskcache</code> for safer cross-process</li> <li>Refactor the shader for future include system external usage</li> <li>Simplify how the ray intersections are computed with ray marching</li> <li>Fix how projection rays are calculated, as <code>steady</code>, <code>focus</code> were slightly wrong</li> <li>Fix base scene duration is now 5 seconds</li> </ul>"},{"location":"about/changelog/#0.8.0","title":"\ud83d\udce6 v0.8.0 October 27, 2024","text":"<ul> <li>Implement batch export logic within the main command line</li> <li>PyTorch is now managed in a top-level CLI entry point</li> <li>Many improvements to the website documentation: Quickstart, examples, and more</li> <li>Added Apple's DepthPro as an Depth Estimator option</li> <li>The exported video now properly follows the input image's resolution</li> <li>Loading inputs is now lazy, and only happens at module setup before rendering</li> <li>Improved the Readme with community work, quick links, showcase</li> </ul>"},{"location":"about/contact/","title":"About/Contact","text":"<ul> <li> <p> Discord <p>Largest community, fastest responses</p> <ul><li>Priority sponsors support</li></ul> Join Server</p> </li> <li> <p> GitHub <p>Officialize your Requests or Issues</p> <ul><li>One repository per project</li></ul> All Projects</p> </li> <li> <p> Telegram <p>Also an option if you prefer over others!</p> <ul><li>Larger uploads can be useful</li></ul> Join Group</p> </li> <li> <p> Email <p>Get in touch directly and privately</p> <ul><li>Business or Traditional</li></ul> Send a Message</p> </li> </ul> <p>I live on the UTC-04:00 Timezone (EDT), so I might be sleeping when you send me a message</p> <p>Feel free to talk in any of the Languages: \ud83c\udde7\ud83c\uddf7 \ud83c\uddfa\ud83c\uddf8</p>"},{"location":"about/roadmap/","title":"Roadmap","text":"<p>     Click here to see the Roadmap on GitHub Projects </p>"},{"location":"docs/","title":"\u26a1\ufe0f Quick start","text":"<p>This section focus on getting started with DepthFlow, demonstrating basic usage, commands, and features. The main goal is to help you create your first 3D parallax effect videos quickly and easily, from the command line, web interface, or custom scripts.</p> <p> Go to any section in the left sidebar to get started!</p> Before continuing: <p>From any installation method, you can simply run <code>depthflow</code> or double click the executables to see all top-most available commands (entry points). You should see something like:</p> <p>You can run any of the commands above plus <code>--help</code> to list all options for that command.</p> <p>For example, <code>depthflow gradio --help</code> will show all options for the web interface, running <code>depthflow --help</code> will show all CLI options (as is the default command).</p>"},{"location":"docs/commands/","title":"Command line","text":"<p>\u2705 As DepthFlow is a ShaderFlow \"spin-off\" - a custom Scene - most of its documentation on commands, behavior, issues and options are shared between the two.</p> <ul> <li>The examples of each section shows a single functionality, but you can combine them.</li> </ul>"},{"location":"docs/commands/#simplest-command","title":"Simplest command","text":"<p>Start a realtime window with the default image and animation with:</p> <pre><code>depthflow main\n</code></pre> <ul> <li>Walk around with W A S D or Left click + Drag</li> <li>Drag and drop image files or URLs to load them</li> <li>Press Tab for a dev menu with some options</li> </ul>"},{"location":"docs/commands/#using-your-images","title":"Using your images","text":"<p>Load an input image, and start the main event loop with:</p> <pre><code># Local images, either relative or absolute paths\ndepthflow input -i ./image.png main\n</code></pre> <pre><code># Remote images, like URLs\ndepthflow input -i https://w.wallhaven.cc/full/2y/wallhaven-2y6wwg.jpg main\n</code></pre> <ul> <li>Note: Make sure the image path exists, relative paths (not starting with <code>C:\\</code> or <code>/</code>) are relative to where the the executable or current working directory of the shell or is.</li> </ul>"},{"location":"docs/commands/#exporting-a-video","title":"Exporting a video","text":"<p>Render 5 seconds of the animation to a video file with default settings with:</p> <pre><code>depthflow main -o ./output.mp4\n</code></pre> <p>See all rendering options with <code>depthflow main --help</code></p>"},{"location":"docs/commands/#resolution","title":"Resolution","text":"<p>The output resolution will match the input image by default. You can pass either <code>--width/-w</code> or <code>--height/-h</code> to force one component and fit the other based on the image's aspect ratio:</p> <pre><code># Renders a 2560x1440 (Quad HD) video\ndepthflow input -i ./image16x9.png main -h 1440\n</code></pre> <pre><code># Width is prioritized, this renders a 500x500 video\ndepthflow input -i ./image1000x1000.png main -w 500 -h 800\n</code></pre>"},{"location":"docs/commands/#looping","title":"Looping","text":"<p>The output video will scale and loop perfectly, with a period set by the <code>--time</code> parameter:</p> <pre><code># 5 second video with 1 loop happening\ndepthflow main -o ./output.mp4 --time 5\n</code></pre> <pre><code># 12 second video with 3 loops happening\ndepthflow main -o ./output.mp4 --time 4 --loops 3\n</code></pre>"},{"location":"docs/commands/#video-encoder","title":"Video encoder","text":"<p>You can also easily change the video encoder:</p> <p>You can see all available codecs in <code>depthflow --help</code> !</p> <pre><code># Configure the H264 codec\ndepthflow h264 --preset veryfast main -o ./output.mp4\n</code></pre> <pre><code># Use the H264 codec with NVENC on a NVIDIA GPU\ndepthflow h264-nvenc main -o ./output.mp4\n</code></pre> <pre><code># Only supported in RTX 4000 and newer GPUs\ndepthflow av1-nvenc main -o ./output.mp4\n</code></pre>"},{"location":"docs/commands/#quality","title":"Quality","text":"<p>The video is eternal, so getting the best render quality even if it takes longer is important. There's a couple of main factors that defines the final video quality:</p> <ol> <li> <p>Resolution: A combination of the input image and the exported video's resolution. Rendering at a higher resolution than the input image will not improve quality.</p> </li> <li> <p>Super Sampling Anti Aliasing: Renders at a higher internal resolution and then downscaling to the output target mitigates edge artifacts and smooths them. The default is 1.2, good quality with 2, best with 4, don't go above it. Uses <code>N^2</code> times more GPU power.</p> </li> <li> <p>Quality parameter: The <code>depthflow main --quality 50</code> parameter defines how accurate the projection's intersections are. A value of 0 is sufficient for subtle movements, and will create \"layers\" artifacts at higher offsets. The default is 50, which is actually overkill for most cases.</p> </li> <li> <p>Depth map: Defines the accuracy of the parallax effect. The default estimator is a state of the art balance of speed, portability, quality, and should be enough.</p> </li> <li> <p>Video codec: The encoder compresses the video from unimaginably impractical sizes of raw data to something manageable. Briefly, CPU encoders yields the best compression, file sizes, and quality, but are slower than GPU encoders, which are \"worse\" in every other aspect. Max quality is seen only on the realtime window, as it is the raw data itself.</p> </li> </ol> <pre><code># The stuff explained above in a command:\ndepthflow main --quality 80 --ssaa 2 -o ./output.mp4\n</code></pre> <pre><code># Extremely slow, but the best quality\ndepthflow main --quality 100 --ssaa 4 -o ./output.mp4\n</code></pre>"},{"location":"docs/commands/#using-an-upscaler","title":"Using an upscaler","text":"<p>Upscale the input image before rendering the video with:</p> <pre><code># Use Upscayl to upscale the image (https://github.com/upscayl/upscayl)\ndepthflow upscayl -m digital-art input -i ./image.png main -o ./output.mp4\n</code></pre> <pre><code># Use Waifu2x to upscale the image (https://github.com/nihui/waifu2x-ncnn-vulkan)\ndepthflow waifu2x input -i ./image.png main -o ./output.mp4\n</code></pre>"},{"location":"docs/commands/#custom-animations","title":"Custom animations","text":"<p>\ud83d\udea7 Animations are work in progress, and will change substantially \ud83d\udea7</p> <p>You can use a couple of high quality presets with:</p> <p>See any of <code>depthflow 'preset' --help</code> for more options!</p> <pre><code># Add a horizontal motion to the camera\ndepthflow horizontal main\n</code></pre> <pre><code># Add a vertical motion to the camera\ndepthflow vertical --linear main\n</code></pre> <pre><code># Add a circular motion to the camera\ndepthflow circle --intensity 0.3 main\n</code></pre> <pre><code># Add a dolly zoom to the camera\ndepthflow dolly --reverse -i 2 main\n</code></pre> <pre><code># Add a zoom-in motion to the camera\ndepthflow zoom main\n</code></pre>"},{"location":"docs/commands/#batch-processing","title":"Batch processing","text":"<p><sup>\u26a0\ufe0f Note: Batch exporting feature is experimental and might have issues!</sup></p>"},{"location":"docs/commands/#selecting-inputs","title":"Selecting inputs","text":"<p>Multiple direct inputs</p> <pre><code># Local paths\ndepthflow input -i ./image1.png -i ./image2.png (...)\n\n# Or even URLs\ndepthflow input -i https://.. -i https://.. (...)\n</code></pre> <pre><code># All file contents of a folder\ndepthflow input -i ./images (...)\n</code></pre> <pre><code># Glob pattern matching\ndepthflow input -i ./images/*.png (...)\n</code></pre>"},{"location":"docs/commands/#exporting","title":"Exporting","text":"<p>Let's assume there are <code>foo.png</code>, <code>bar.png</code>, and <code>baz.png</code> in the <code>./images</code> folder:</p> <ol> <li>Always have <code>-b all</code> or <code>--batch all</code> in the <code>main</code> command (or a range like <code>0-5</code> images)</li> <li>The output video basename will become a suffix of the exported video</li> </ol> <pre><code># This creates 'foo-suffix.mp4', 'bar-suffix.mp4', 'baz-suffix.mp4' in the './outputs' folder\ndepthflow input -i ./images main -b all -o ./outputs/suffix\n</code></pre> <p>The prefix is enforced mainly as there's no 'empty' file in a directory, but also useful in:</p> <pre><code># Create many different animations of the same image\ndepthflow input -i ./images orbital main -b all -o ./outputs/orbital\ndepthflow input -i ./images circle  main -b all -o ./outputs/circle\n</code></pre> <p>Or even set the output folder to the same input, so videos sorts nicely alongside images:</p> <pre><code>depthflow input -i ./images main -b all -o ./images\n</code></pre> <p>It might be a good idea to specify a common height for all exports:</p> <pre><code># Ensures all videos are '1080p', at least in the height\ndepthflow input -i ./images main -b all -o ./images -h 1080p\n</code></pre> <p>Loving what you see? Help me continue this Full-Time Open Source Work!</p>"},{"location":"docs/parameters/","title":"DepthFlow/Parameters","text":"<p>This page focus on all shader-related parameters, and how they affect the final image (in practical terms). The main goal is for helping you to understand how to work with the software and parametrize it to your needs, create your own animations, and more.</p> <ul> <li>For rendering and exporting parameters, see the ShaderFlow page \u2728</li> <li>For understanding the math, see the Foundations page \ud83d\udcdc</li> </ul> <p>All parameters are controlled within the state dictionary class, acessible by:</p> <pre><code>from depthflow import DepthScene\n\nscene = DepthScene()\nscene.state.height = 0.3\nscene.state.offset_x = 1\n</code></pre> <p>Or settable by the command line as a animation component:</p> Terminal<pre><code># Note: This will only create a static image\ndepthflow state --height 0.3 --offset-x 1 main\n</code></pre> <p>However, they are best used within the <code>.update()</code> method for creating animations:</p> <pre><code>import math\n\nclass YourScene(DepthScene):\n    def update(self):\n        self.state.offset_x = 0.3 * math.sin(self.cycle)\n</code></pre> <p>Internally, they are used and sent when the render method is called for the shader, which happens in the main event loop of the Scene, after all <code>.update()</code>'s.</p> <p>Directly controlling those makes more sense when managing the code within Python (and bypassing the animation system, which essentially does the same thing) for writing custom animations and more advanced interactions or behaviors.</p>"},{"location":"docs/parameters/#parallax","title":"Parallax","text":"<p>This section is about the core parameters of DepthFlow.</p> <ul> <li> <p>Important: When parameters refers to depth values, the value is normalized. A <code>steady</code> value of \\(0.5\\), with a <code>height</code> of \\(0.3\\) means the perceptual <code>steady</code> values is at \\(0.15\\), (e.g.).</p> </li> <li> <p>Important: Depth values of zero are the farthest point, and values of one the closest.</p> </li> <li> <p>Note: The suggested ranges aren't enforced, but what makes sense in most cases.</p> </li> </ul>"},{"location":"docs/parameters/#height","title":"Height","text":"<p>Type: <code>float</code>, Names: <code>height</code>, Range: <code>[0, 1]</code></p> <p>The <code>height</code> parameter defines the peak height of the projected surface at <code>depth=1</code>. It can be thought as the effect's global intensity parameter.</p> <p>It's arguably the most important parameter, virtually nothing happens without it</p> <ul> <li> <p>A value of 0 means the surface is flat, while a value of 1 means the surface's peak is on the same \\(xy\\) screen plane as the camera.</p> </li> <li> <p>Notice how in the video, the center doesn't touch the camera, as its <code>depth</code> value isn't \\(1\\), but the closer bottom edge \"gets below\" the camera's view.</p> </li> </ul>"},{"location":"docs/parameters/#offset","title":"Offset","text":"<p>Type: <code>tuple[float, float]</code>, Names: <code>offset_x</code>, <code>offset_y</code>, Range: <code>[-2, 2]</code></p> <p>The <code>offset</code> parameter defines the parallax displacement of the projected surface. It can be thought as the camera's position parameter.</p> <p>This is the easiest way to add 'natural' movement to the scene</p> <ul> <li> <p>A value of 0 in a component means the surface and camera are centered, other values meaning depends on other parameters and the aspect ratio, it's a bit experimental.</p> </li> <li> <p>This parameter isn't a \"camera displacement\" you might expect:</p> <ol> <li>That would simply move the image around without changing the perspective, which is what the <code>center</code> parameter does.</li> <li>The camera always \"looks\" to the image (<code>origin</code> parameter) by adding an opposite bias to the ray's projection on how much the image is displaced.</li> </ol> </li> </ul> <p>As you might expect, setting \\(x=cos(t)\\) and \\(y=sin(t)\\) parameter to follow a circular motion, will create a \"orbiting\" effect around the center of the image.</p>"},{"location":"docs/parameters/#steady","title":"Steady","text":"<p>Type: <code>float</code>, Names: <code>steady</code>, Range: <code>[-1, 1]</code></p> <p>The <code>steady</code> parameter defines the depth at which no offsets happen. It can be thought as the offsets focal depth parameter.</p> <p>It's a great way of adding subtle background movement or orbiting around a point</p> <ul> <li> <p>Notice how in the video, the orange line doesn't move when the <code>offset</code> changes, and the mirroring of relative directions when crossing this boundary.</p> </li> <li> <p>This parameter makes the ray projections \"pivot\" around this depth value internally.</p> </li> </ul>"},{"location":"docs/parameters/#isometric","title":"Isometric","text":"<p>Type: <code>float</code>, Names: <code>isometric</code>, Range: <code>[0, 1]</code></p> <p>The <code>isometric</code> parameter defines how much perspective is applied. It can be thought as the planification effect parameter.</p> <p>It's the best way to mitigate edge or stretching distortions, at the cost of the 3D-ness of the video</p> <ul> <li> <p>A value of 0 means full perspective projection, while a value of 1 means the image is projected as if it was isometric (all rays are parallel).</p> <ol> <li>This completely negates the <code>height</code> parameter at <code>isometric=1</code></li> </ol> </li> <li> <p>This parameter makes effect more \"flat\" and \"2D\", in fact, a value of 1 turns offsets into a simple translation. A value of 0.5 is often recommended.</p> </li> </ul> <p>Notice how in the video below the offsets are \"flattened\", as if there was one layer per depth value and it was simply displaced in the \\(xy\\) plane. Consequently, more of the image is visible, as the peak values don't race towards the camera as much, at the cost of being flat.</p>"},{"location":"docs/parameters/#dolly","title":"Dolly","text":"<p>Type: <code>float</code>, Names: <code>dolly</code>, Range: <code>[0, 10]</code></p> <p>The <code>dolly</code> parameter defines the camera's distance from the image. It's basically the same as the isometric effect parameter, but with different (natural) units.</p> <p>It's a great way for a more natural isometric effect control</p> <ul> <li>As you move away to objects, they appear more isometric, that's the reason why your face looks unnatural in close-up selfies.</li> <li>A <code>dolly</code> value of 0 is the same as <code>isometric=0</code></li> <li>A <code>dolly</code> value of \\(\\infty\\) is the same as <code>isometric=1</code></li> </ul> <p>As far as I know, the convertion factor between the two is given by:</p> \\[ \\text{isometric} = 1 - \\frac{1}{1 + \\text{dolly}} \\] <p>For the traditional 'dolly zoom' effect, combine it with the <code>focus</code> parameter.</p>"},{"location":"docs/parameters/#focus","title":"Focus","text":"<p>Type: <code>float</code>, Names: <code>focus</code>, Range: <code>[-1, 1]</code></p> <p>The <code>focus</code> parameter defines the steady depth on isometric changes. It can be thought as the isometric focal depth parameter.</p> <p>It's a great way to add drama to the scene, or give attention to an object</p> <ul> <li> <p>Notice how in the video, the orange line doesn't move when the <code>isometric</code> changes, and the mirroring of perspective directions when crossing this boundary.</p> </li> <li> <p>This parameter makes this depth value the \\(z=1\\) plane internally.</p> </li> </ul>"},{"location":"docs/parameters/#zoom","title":"Zoom","text":"<p>Type: <code>float</code>, Names: <code>zoom</code>, Range: <code>(0, 1]</code></p> <p>The <code>zoom</code> parameter defines the camera's field of view. It can be thought as the you-know-it parameter.</p> <p>It's a great way to crop the image</p> <ul> <li> <p>A value of 1 means the image is fully visible, while a value of 0.5 means a quarter of the image is visible.</p> </li> <li> <p>This is a \"digital zoom\", it simply stretches the coordinates internally.</p> </li> </ul>"},{"location":"docs/parameters/#invert","title":"Invert","text":"<p>Type: <code>float</code>, Names: <code>invert</code>, Range: <code>[0, 1]</code></p> <p>The <code>invert</code> parameter interpolates between 0=far and 1=near and the opposite. It can be thought as the depth inversion parameter.</p> <p>This parameter is mostly useful when the input depth map is inverted</p> <ul> <li> <p>A value of 0.5 flattens the depth map and nothing happens, while a value of 1 inverts the depth map. Middle values can be thought as softening the depthmap.</p> </li> <li> <p>It wraps the surface inside-out when the value is above 0.5, and a lot of encroaching will happen, as the background is now the foreground.</p> </li> </ul>"},{"location":"docs/parameters/#center","title":"Center","text":"<p>Type: <code>tuple[float, float]</code>, Names: <code>center_x</code>, <code>center_y</code>, Range: <code>([-ar, ar], [-1, 1])</code></p> <p>The <code>center</code> parameter defines the center of the image. It can be thought as the raw offset parameter.</p> <p>This is the easiest way to move the image around</p> <ul> <li> <p>A value of 0 in a component means the image is centered, other values applies a direct offset to the contents of the image.</p> </li> <li> <p>This parameter is a \"camera displacement\" you might expect, nothing fancy.</p> </li> </ul>"},{"location":"docs/parameters/#origin","title":"Origin","text":"<p>Type: <code>tuple[float, float]</code>, Names: <code>origin_x</code>, <code>origin_y</code>, Range: <code>([-ar, ar], [-1, 1])</code></p> <p>The <code>origin</code> parameter defines the center point of offsets. It can be thought  as if the camera was above this point , without moving it.</p> <p>This is a good way to focus on a specific part of the image while feeling off-center</p> <ul> <li> <p>The value sets \"the origin\" of offsets to the projections of the image.</p> </li> <li> <p>It is also the value at which height changes only causes zooming</p> </li> </ul>"},{"location":"docs/parameters/#depth-of-field","title":"Depth of Field","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p>"},{"location":"docs/parameters/#vignette","title":"Vignette","text":"<p>\ud83d\udea7 Work in Progress \ud83d\udea7</p>"},{"location":"docs/scripts/","title":"Python scripts","text":"<p>     Click here to see example scripts on GitHub </p>"},{"location":"docs/webui/","title":"Gradio WebUI","text":"<p>\u2705 On any installation method, you can run <code>depthflow gradio</code> to start the gradio web interface. This will open a new tab in your default browser hosted in your local machine.</p> <p></p> <p>\ud83d\udcc1 Input any image on the top left, optionally upscale it; change the animations and/or configure rendering options; hit \"Render\" to generate a video!</p> <p>\u267b\ufe0f You can pass a <code>depthflow gradio --share</code> flag to have a temporary public link of your instance, proxied through Gradio's servers. Do this when hosting for a friend, or if you don't have access to the local network of a remote/rented server!</p> <p>Loving what you see? Help me continue this Full-Time Open Source Work!</p>"},{"location":"examples/readme/","title":"Readme","text":"<p>This directory contains example scripts to run DepthFlow, from basics to advanced usage.</p> <ul> <li>Warn: These are only guaranteed to run against the latest <code>git main</code> code.</li> </ul>"},{"location":"get/","title":"Installation","text":""},{"location":"get/#from-wheels","title":"\ud83e\uddc0 From Wheels","text":"<p> <p>The most reliable way to use the projects \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f</p> <ul> <li>Run directly with astral-sh/uv tools or use as a package</li> <li>Run the package commands or as a python import</li> </ul> <p><sup>Recommended for: Basic users, advanced users, developers</sup></p> <p></p>"},{"location":"get/#from-source","title":"\ud83d\udd25 From Source","text":"<p> <p>The most flexible way to use the projects \u2b50\ufe0f\u2b50\ufe0f</p> <ul> <li>Automatic install scripts, spend more time using the projects</li> <li>Latest features, bugs, fixes, highly configurable</li> </ul> <p><sup>Recommended for: Advanced users, contributors, developers</sup></p> <p></p>"},{"location":"get/installers/","title":"Installers","text":"Windows Linux MacOS <p> Note: Executables are safe and auditable, but might trigger false antivirus alerts     I will not destroy my reputation by distributing malware, code signing is infeasible. </p> x86-64 <p></p> <p> Note: Open a terminal in the download path, extract it with <code>cat *.tar.gz | tar -xzvf - -i</code> <sup>And then run <code>./project-name-*.bin</code> shown in the previous output for executing it!</sup> </p> x86-64ARM64 <p></p> <p></p> <p> Note: Open a terminal in the download path, extract it with <code>cat *.tar.gz | tar -xzvf - -i</code> <sup>And then run <code>./project-name-*.bin</code> shown in the previous output for executing it!</sup> </p> Apple SiliconIntel Macs <p></p> <p></p> <p><sup> \u2764\ufe0f Note: You can contact me for a free copy of a paid executable with a valid or altruistic reason!</sup></p>"},{"location":"get/installers/#uninstalling","title":"\u267b\ufe0f Uninstalling","text":"<p>Workspaces: Where data, models, versions, cache of the projects are stored</p> <p>All project's Workspaces are located in your platform's <code>User Data</code> directory:   Windows C:\\Users\\(...)\\AppData\\Local\\BrokenSource  MacOS ~/Library/Application Support/BrokenSource  Linux ~/.local/share/BrokenSource </p> <p>Note: This should be the only directory used by Installers</p>"},{"location":"get/source/","title":"Source","text":"Windows Linux MacOS Manual <p> Open a folder to download the code on Windows Explorer Press Ctrl+L , run <code>powershell</code> and execute: <pre><code>irm https://brokensrc.dev/get.ps1 | iex\n</code></pre> How it works: <code>irm</code> downloads the script, <code>iex</code> executes it directly  Don't want to use it? Follow the  Manual tab above!</p> <p> Open a Terminal on some directory and run: <pre><code>/bin/bash -c \"$(curl -sS https://brokensrc.dev/get.sh)\"\n</code></pre> How it works: <code>curl</code> downloads the script, <code>bash</code> executes it directly  Don't want to use it? Follow the  Manual tab above!</p> <p> Open a Terminal on some directory and run: <pre><code>/bin/bash -c \"$(curl -sS https://brokensrc.dev/get.sh)\"\n</code></pre> How it works: <code>curl</code> downloads the script, <code>bash</code> executes it directly  Don't want to use it? Follow the  Manual tab above!</p> <p></p> <ul> <li>Install git and uv on your Platform</li> </ul> <p>Download the code<pre><code>git clone https://github.com/BrokenSource/BrokenSource --recurse-submodules\n</code></pre> Enter the directory<pre><code>cd BrokenSource\n</code></pre> Ensure submodules are on main<pre><code>git submodule foreach --recursive 'git checkout main || true'\n</code></pre> Create venv and install dependencies<pre><code>uv sync --all-packages\n</code></pre></p> Directly with uvTraditional method <p></p> Start using any Project<pre><code>uv run shaderflow\nuv run depthflow\nuv run broken\n</code></pre> <p></p> <p>Activate the venv<pre><code># Windows:\n.venv\\Scripts\\Activate.ps1 # PowerShell\n.venv\\Scripts\\Activate.bat # CMD\n\n# Linux and MacOS:\nsource .venv/bin/activate # Bash\nsource .venv/bin/activate.fish # Fish\n</code></pre> Start using any Project<pre><code>broken\nshaderflow\ndepthflow\n</code></pre></p>"},{"location":"get/source/#uninstalling","title":"\u267b\ufe0f Uninstalling","text":"<p>Apart from deleting the <code>BrokenSource</code> folder where you cloned the code,</p> <p>Workspaces: Where data, models, versions, cache of the projects are stored</p> <p>All project's Workspaces are located in your platform's <code>User Data</code> directory:   Windows C:\\Users\\(...)\\AppData\\Local\\BrokenSource  MacOS ~/Library/Application Support/BrokenSource  Linux ~/.local/share/BrokenSource </p> <p>Models: PyTorch, HuggingFace, TorchHub downloads models on:</p> <p>  Windows C:\\Users\\(...)\\.cache\\{huggingface,transformers,torch}  MacOS ~/Library/Caches/{huggingface,transformers,torch}  Linux ~/.cache/{huggingface,transformers,torch} </p> <p>Package manager cache: </p>  uv Pip Poetry PDM <p>Either run <code>uv cache prune</code> or <code>uv cache dir</code> to locate wheel downloads caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\uv  MacOS ~/Library/Caches/uv  Linux ~/.cache/uv </p> <p>Either run <code>pip cache purge</code> or <code>pip cache dir</code> to locate wheel download caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pip  MacOS ~/Library/Caches/pip  Linux ~/.cache/pip </p> <p>Either run <code>poetry cache clear</code> or <code>poetry cache list</code> to locate caches and venvs, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pypoetry  MacOS ~/Library/Caches/pypoetry  Linux ~/.cache/pypoetry </p> <p>Either run <code>pdm cache clear</code> or <code>pdm cache list</code> to locate wheel downloads caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pdm  MacOS ~/Library/Caches/pdm  Linux ~/.cache/pdm </p>"},{"location":"get/wheels/","title":"Wheels","text":"Directly Package <p>  1. Open a Terminal and install astral-sh/uv - a fast python and project manager:</p>  Windows Linux MacOS <p>Using WinGet, Microsoft's official package manager: <pre><code>winget install --id=astral-sh.uv -e\n</code></pre></p> <p>Install from your distro package manager, or universally: <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>Using Homebrew, a popular package manager for MacOS: <pre><code>brew install uv\n</code></pre></p> <p> 2. Run any project simply with: <pre><code>uvx (project) (args)\n</code></pre>  For example <code>uvx depthflow gradio</code></p> <p>  Add any project to your <code>pyproject.toml</code> or install inside a venv, write and run any scripts.</p> <ul> <li>Check the examples tabs at the top or the repository for usage!</li> </ul> <p>Preferably pin the package version <code>==x.y.z</code> anywhere for stability!</p>"},{"location":"get/wheels/#uninstalling","title":"\u267b\ufe0f Uninstalling","text":"<p>Apart from uninstalling the package and/or deleting the virtual environment:</p> <p>Workspaces: Where data, models, versions, cache of the projects are stored</p> <p>All project's Workspaces are located in your platform's <code>User Data</code> directory:   Windows C:\\Users\\(...)\\AppData\\Local\\BrokenSource  MacOS ~/Library/Application Support/BrokenSource  Linux ~/.local/share/BrokenSource </p> <p>Models: PyTorch, HuggingFace, TorchHub downloads models on:</p> <p>  Windows C:\\Users\\(...)\\.cache\\{huggingface,transformers,torch}  MacOS ~/Library/Caches/{huggingface,transformers,torch}  Linux ~/.cache/{huggingface,transformers,torch} </p> <p>Package manager cache: </p>  uv Pip Poetry PDM <p>Either run <code>uv cache prune</code> or <code>uv cache dir</code> to locate wheel downloads caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\uv  MacOS ~/Library/Caches/uv  Linux ~/.cache/uv </p> <p>Either run <code>pip cache purge</code> or <code>pip cache dir</code> to locate wheel download caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pip  MacOS ~/Library/Caches/pip  Linux ~/.cache/pip </p> <p>Either run <code>poetry cache clear</code> or <code>poetry cache list</code> to locate caches and venvs, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pypoetry  MacOS ~/Library/Caches/pypoetry  Linux ~/.cache/pypoetry </p> <p>Either run <code>pdm cache clear</code> or <code>pdm cache list</code> to locate wheel downloads caches, should be at:   Windows C:\\Users\\(...)\\AppData\\Local\\pdm  MacOS ~/Library/Caches/pdm  Linux ~/.cache/pdm </p>"}]}